{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "vecmodel = KeyedVectors.load_word2vec_format('../../../GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras import backend as K\n",
    "from scipy.stats.stats import pearsonr   \n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entailment_judgment</th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entailment_judgment  pair_ID  relatedness_score  \\\n",
       "0             NEUTRAL      1.0                4.5   \n",
       "1             NEUTRAL      2.0                3.2   \n",
       "2          ENTAILMENT      3.0                4.7   \n",
       "3             NEUTRAL      5.0                3.4   \n",
       "4             NEUTRAL      9.0                3.7   \n",
       "\n",
       "                                          sentence_A  \\\n",
       "0  A group of kids is playing in a yard and an ol...   \n",
       "1  A group of children is playing in the house an...   \n",
       "2  The young boys are playing outdoors and the ma...   \n",
       "3  The kids are playing outdoors near a man with ...   \n",
       "4  The young boys are playing outdoors and the ma...   \n",
       "\n",
       "                                          sentence_B  \n",
       "0  A group of boys in a yard is playing and a man...  \n",
       "1  A group of kids is playing in a yard and an ol...  \n",
       "2  The kids are playing outdoors near a man with ...  \n",
       "3  A group of kids is playing in a yard and an ol...  \n",
       "4  A group of kids is playing in a yard and an ol...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = \"['pair_ID', 'sentence_A', 'sentence_B', 'relatedness_score','entailment_judgment']\"\n",
    "train_df = pd.read_csv(\"../data/enhance_traindata.csv\", sep='\\t')\n",
    "trial_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_trial.txt\", sep='\\t')\n",
    "test_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_test_annotated.txt\", sep='\\t')\n",
    "\n",
    "texts = []\n",
    "BASE_DIR = ''\n",
    "TEXT_DATA_DIR = os.path.join('../data/SemEval2014_dataset/')\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create word_id\n",
    "merged = train_df['sentence_A'].tolist()\n",
    "merged.extend(train_df['sentence_B'].tolist())\n",
    "merged.extend(trial_df['sentence_A'].tolist())\n",
    "merged.extend(trial_df['sentence_B'].tolist())\n",
    "merged.extend(test_df['sentence_A'].tolist())\n",
    "merged.extend(test_df['sentence_B'].tolist())\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(merged)\n",
    "sequences = tokenizer.texts_to_sequences(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "# prepare embedding matrix\n",
    "num_words = max(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in vecmodel.vocab:\n",
    "        embedding_vector = vecmodel[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenized_padded_data(data):\n",
    "    \n",
    "#     t = Tokenizer(num_words=MAX1_NB_WORDS)\n",
    "#     t.fit_on_texts(data)\n",
    "#     r = RegexpTokenizer(r'\\w+')\n",
    "#     r.tokenize(data)\n",
    "    padded_data = []\n",
    "    for d in data:\n",
    "        r = RegexpTokenizer(r'\\w+')\n",
    "        c = r.tokenize(d)\n",
    "        seq = tokenizer.texts_to_sequences(c)\n",
    "        seq = [item for sublist in seq for item in sublist]\n",
    "        padded_data.append(seq)    \n",
    "    padded_data = pad_sequences(padded_data, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "    return padded_data\n",
    "\n",
    "# get_tokenized_padded_data(['The young \"boys\" @ are playing outdoors and the man is smiling nearby','Nobody is riding the bicycle on one wheel'])\n",
    "\n",
    "def get_target_category(data):\n",
    "    \n",
    "    target= np.zeros([len(data),6])\n",
    "    for i in range(len(data)):\n",
    "        value = int(round(data[i]))\n",
    "        target[i][value] = 1\n",
    "    return target\n",
    "\n",
    "def pear_coef(y_true, y_pred):\n",
    "    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)\n",
    "    return pearson_r\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "   \n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = (x - mx), (y-my)\n",
    "    r_num = K.sum(xm * ym)\n",
    "    r_den = K.sqrt(K.sum(K.square(xm))) * K.sqrt(K.sum(K.square(ym)))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   1  87   6 219   3  16\n",
      "   4   1 616   5  25 285  10   3  42   4   2 402]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xA_train = get_tokenized_padded_data(train_df['sentence_A'].tolist())\n",
    "print(xA_train[0])\n",
    "xA_val = get_tokenized_padded_data(trial_df['sentence_A'].tolist())\n",
    "xA_test = get_tokenized_padded_data(test_df['sentence_A'].tolist())\n",
    "\n",
    "xB_train = get_tokenized_padded_data(train_df['sentence_B'].tolist())\n",
    "xB_val = get_tokenized_padded_data(trial_df['sentence_B'].tolist())\n",
    "xB_test = get_tokenized_padded_data(test_df['sentence_B'].tolist())\n",
    "\n",
    "y_train = get_target_category(train_df['relatedness_score'].tolist())\n",
    "y_val = get_target_category(trial_df['relatedness_score'].tolist())\n",
    "y_test = get_target_category(test_df['relatedness_score'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15065, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15065, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xA_train.shape)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inp = Input(shape=(MAX_SEQUENCE_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(num_words + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seqA_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "# embedding_layer = Embedding(num_words,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=False)\n",
    "\n",
    "# print (seqA_input.shape)\n",
    "# x_A = embedding_layer(seqA_input)\n",
    "# print (x_A.shape)\n",
    "# x_A = Convolution2D(filters=300,\n",
    "#                          kernel_size=[ 1 , 1 ],\n",
    "#                          padding=\"valid\",\n",
    "#                          activation=\"relu\",\n",
    "#                          strides=1)(x_A)\n",
    "\n",
    "# print (x_A.shape)\n",
    "# x_A = MaxPooling2D(pool_size=[MAX_SEQUENCE_LENGTH,1])(x_A)\n",
    "\n",
    "# # x_B = embedding_layer(sequence_input)\n",
    "# # x_B = Convolution2D(filters=300,\n",
    "# #                          kernel_size=[ 1 , 1 ],\n",
    "# #                          padding=\"valid\",\n",
    "# #                          activation=\"relu\",\n",
    "# #                          strides=1)(x_A)\n",
    "\n",
    "# # x_B = MaxPooling2D(pool_size=[MAX_SEQUENCE_LENGTH,1])(x_B)\n",
    "\n",
    "\n",
    "# x_A = Dense(300, activation='relu')(x_A)\n",
    "# preds = Dense(5, activation='softmax')(x_A)\n",
    "# model = Model(seqA_input, preds)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "# model.summary()\n",
    "# # model.fit(xA_train, y_train,\n",
    "# #           batch_size=128,\n",
    "# #           epochs=10,\n",
    "# #           validation_data=(xA_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30)\n"
     ]
    }
   ],
   "source": [
    "seqA_input = Input(shape=(MAX_SEQUENCE_LENGTH,),)\n",
    "seqB_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print (seqA_input.shape)\n",
    "x_A = embedding_layer(seqA_input)\n",
    "x_A = Conv1D(filters=300,kernel_size=1,padding=\"valid\",activation=\"relu\",strides=1,kernel_initializer='he_uniform')(x_A)\n",
    "x_A = MaxPooling1D(pool_size=[MAX_SEQUENCE_LENGTH])(x_A)\n",
    "\n",
    "x_B = embedding_layer(seqB_input)\n",
    "x_B = Conv1D(filters=300,kernel_size=1,padding=\"valid\",activation=\"relu\",strides=1,kernel_initializer='he_uniform')(x_B)\n",
    "x_B = MaxPooling1D(pool_size=[MAX_SEQUENCE_LENGTH])(x_B)\n",
    "\n",
    "diff = keras.layers.Subtract()([x_A, x_B])\n",
    "prod = keras.layers.Multiply()([x_A, x_B])\n",
    "\n",
    "\n",
    "x = keras.layers.concatenate([diff, prod])\n",
    "\n",
    "\n",
    "x = Dense(300, activation='tanh')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "preds = Dense(6, activation='softmax')(x)\n",
    "model = Model(inputs=[seqA_input,seqB_input], outputs=preds)\n",
    "\n",
    "\n",
    "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=[correlation_coefficient])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15065 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "15065/15065 [==============================] - 11s 704us/step - loss: 0.1237 - correlation_coefficient: 0.3268 - val_loss: 0.1229 - val_correlation_coefficient: 0.3409\n",
      "Epoch 2/50\n",
      "15065/15065 [==============================] - 12s 770us/step - loss: 0.1104 - correlation_coefficient: 0.4535 - val_loss: 0.1235 - val_correlation_coefficient: 0.3446\n",
      "Epoch 3/50\n",
      "15065/15065 [==============================] - 12s 772us/step - loss: 0.0995 - correlation_coefficient: 0.5340 - val_loss: 0.1284 - val_correlation_coefficient: 0.3208\n",
      "Epoch 4/50\n",
      "15065/15065 [==============================] - 12s 774us/step - loss: 0.0883 - correlation_coefficient: 0.6053 - val_loss: 0.1328 - val_correlation_coefficient: 0.3074\n",
      "Epoch 5/50\n",
      "15065/15065 [==============================] - 12s 793us/step - loss: 0.0788 - correlation_coefficient: 0.6589 - val_loss: 0.1360 - val_correlation_coefficient: 0.2862\n",
      "Epoch 6/50\n",
      "15065/15065 [==============================] - 12s 774us/step - loss: 0.0701 - correlation_coefficient: 0.7048 - val_loss: 0.1401 - val_correlation_coefficient: 0.2859\n",
      "Epoch 7/50\n",
      "15065/15065 [==============================] - 12s 810us/step - loss: 0.0614 - correlation_coefficient: 0.7481 - val_loss: 0.1388 - val_correlation_coefficient: 0.3171\n",
      "Epoch 8/50\n",
      "15065/15065 [==============================] - 11s 718us/step - loss: 0.0559 - correlation_coefficient: 0.7735 - val_loss: 0.1449 - val_correlation_coefficient: 0.3005\n",
      "Epoch 9/50\n",
      "15065/15065 [==============================] - 11s 703us/step - loss: 0.0500 - correlation_coefficient: 0.8002 - val_loss: 0.1473 - val_correlation_coefficient: 0.2889\n",
      "Epoch 10/50\n",
      "15065/15065 [==============================] - 13s 858us/step - loss: 0.0464 - correlation_coefficient: 0.8161 - val_loss: 0.1509 - val_correlation_coefficient: 0.2799\n",
      "Epoch 11/50\n",
      "15065/15065 [==============================] - 13s 894us/step - loss: 0.0427 - correlation_coefficient: 0.8321 - val_loss: 0.1567 - val_correlation_coefficient: 0.2755\n",
      "Epoch 12/50\n",
      "15065/15065 [==============================] - 12s 785us/step - loss: 0.0404 - correlation_coefficient: 0.8419 - val_loss: 0.1530 - val_correlation_coefficient: 0.2977\n",
      "Epoch 13/50\n",
      "15065/15065 [==============================] - 12s 816us/step - loss: 0.0374 - correlation_coefficient: 0.8550 - val_loss: 0.1633 - val_correlation_coefficient: 0.2549\n",
      "Epoch 14/50\n",
      "15065/15065 [==============================] - 11s 704us/step - loss: 0.0372 - correlation_coefficient: 0.8558 - val_loss: 0.1631 - val_correlation_coefficient: 0.2439\n",
      "Epoch 15/50\n",
      "15065/15065 [==============================] - 10s 696us/step - loss: 0.0349 - correlation_coefficient: 0.8650 - val_loss: 0.1586 - val_correlation_coefficient: 0.2861\n",
      "Epoch 16/50\n",
      "15065/15065 [==============================] - 11s 699us/step - loss: 0.0322 - correlation_coefficient: 0.8765 - val_loss: 0.1591 - val_correlation_coefficient: 0.2899\n",
      "Epoch 17/50\n",
      "15065/15065 [==============================] - 11s 740us/step - loss: 0.0310 - correlation_coefficient: 0.8813 - val_loss: 0.1617 - val_correlation_coefficient: 0.2892\n",
      "Epoch 18/50\n",
      "15065/15065 [==============================] - 13s 832us/step - loss: 0.0313 - correlation_coefficient: 0.8801 - val_loss: 0.1640 - val_correlation_coefficient: 0.2729\n",
      "Epoch 19/50\n",
      "15065/15065 [==============================] - 11s 746us/step - loss: 0.0291 - correlation_coefficient: 0.8891 - val_loss: 0.1662 - val_correlation_coefficient: 0.2722\n",
      "Epoch 20/50\n",
      "15065/15065 [==============================] - 13s 884us/step - loss: 0.0286 - correlation_coefficient: 0.8911 - val_loss: 0.1653 - val_correlation_coefficient: 0.2777\n",
      "Epoch 21/50\n",
      "15065/15065 [==============================] - 12s 777us/step - loss: 0.0273 - correlation_coefficient: 0.8963 - val_loss: 0.1692 - val_correlation_coefficient: 0.2633\n",
      "Epoch 22/50\n",
      "15065/15065 [==============================] - 11s 734us/step - loss: 0.0263 - correlation_coefficient: 0.9003 - val_loss: 0.1694 - val_correlation_coefficient: 0.2810\n",
      "Epoch 23/50\n",
      "15065/15065 [==============================] - 11s 708us/step - loss: 0.0253 - correlation_coefficient: 0.9043 - val_loss: 0.1665 - val_correlation_coefficient: 0.2722\n",
      "Epoch 24/50\n",
      "15065/15065 [==============================] - 12s 822us/step - loss: 0.0250 - correlation_coefficient: 0.9056 - val_loss: 0.1657 - val_correlation_coefficient: 0.2863\n",
      "Epoch 25/50\n",
      "15065/15065 [==============================] - 11s 715us/step - loss: 0.0243 - correlation_coefficient: 0.9085 - val_loss: 0.1650 - val_correlation_coefficient: 0.2947\n",
      "Epoch 26/50\n",
      "15065/15065 [==============================] - 11s 712us/step - loss: 0.0250 - correlation_coefficient: 0.9056 - val_loss: 0.1675 - val_correlation_coefficient: 0.3038\n",
      "Epoch 27/50\n",
      "15065/15065 [==============================] - 10s 696us/step - loss: 0.0267 - correlation_coefficient: 0.8990 - val_loss: 0.1761 - val_correlation_coefficient: 0.2402\n",
      "Epoch 28/50\n",
      "15065/15065 [==============================] - 11s 712us/step - loss: 0.0244 - correlation_coefficient: 0.9079 - val_loss: 0.1734 - val_correlation_coefficient: 0.2672\n",
      "Epoch 29/50\n",
      "15065/15065 [==============================] - 11s 754us/step - loss: 0.0232 - correlation_coefficient: 0.9126 - val_loss: 0.1719 - val_correlation_coefficient: 0.2731\n",
      "Epoch 30/50\n",
      "15065/15065 [==============================] - 14s 903us/step - loss: 0.0228 - correlation_coefficient: 0.9142 - val_loss: 0.1707 - val_correlation_coefficient: 0.2705\n",
      "Epoch 31/50\n",
      "15065/15065 [==============================] - 11s 750us/step - loss: 0.0220 - correlation_coefficient: 0.9174 - val_loss: 0.1713 - val_correlation_coefficient: 0.2796\n",
      "Epoch 32/50\n",
      "15065/15065 [==============================] - 11s 754us/step - loss: 0.0219 - correlation_coefficient: 0.9180 - val_loss: 0.1737 - val_correlation_coefficient: 0.2720\n",
      "Epoch 33/50\n",
      "15065/15065 [==============================] - 12s 766us/step - loss: 0.0219 - correlation_coefficient: 0.9178 - val_loss: 0.1731 - val_correlation_coefficient: 0.2805\n",
      "Epoch 34/50\n",
      "15065/15065 [==============================] - 11s 744us/step - loss: 0.0200 - correlation_coefficient: 0.9251 - val_loss: 0.1700 - val_correlation_coefficient: 0.2896\n",
      "Epoch 35/50\n",
      "15065/15065 [==============================] - 11s 743us/step - loss: 0.0206 - correlation_coefficient: 0.9229 - val_loss: 0.1773 - val_correlation_coefficient: 0.2611\n",
      "Epoch 36/50\n",
      "15065/15065 [==============================] - 11s 701us/step - loss: 0.0197 - correlation_coefficient: 0.9263 - val_loss: 0.1728 - val_correlation_coefficient: 0.2852\n",
      "Epoch 37/50\n",
      "15065/15065 [==============================] - 12s 804us/step - loss: 0.0201 - correlation_coefficient: 0.9249 - val_loss: 0.1738 - val_correlation_coefficient: 0.2771\n",
      "Epoch 38/50\n",
      "15065/15065 [==============================] - 11s 762us/step - loss: 0.0194 - correlation_coefficient: 0.9277 - val_loss: 0.1741 - val_correlation_coefficient: 0.2703\n",
      "Epoch 39/50\n",
      "15065/15065 [==============================] - 13s 873us/step - loss: 0.0195 - correlation_coefficient: 0.9273 - val_loss: 0.1704 - val_correlation_coefficient: 0.2880\n",
      "Epoch 40/50\n",
      "15065/15065 [==============================] - 12s 765us/step - loss: 0.0200 - correlation_coefficient: 0.9254 - val_loss: 0.1756 - val_correlation_coefficient: 0.2757\n",
      "Epoch 41/50\n",
      "15065/15065 [==============================] - 11s 716us/step - loss: 0.0192 - correlation_coefficient: 0.9284 - val_loss: 0.1747 - val_correlation_coefficient: 0.2839\n",
      "Epoch 42/50\n",
      "15065/15065 [==============================] - 11s 701us/step - loss: 0.0187 - correlation_coefficient: 0.9302 - val_loss: 0.1736 - val_correlation_coefficient: 0.2848\n",
      "Epoch 43/50\n",
      "15065/15065 [==============================] - 11s 700us/step - loss: 0.0186 - correlation_coefficient: 0.9307 - val_loss: 0.1736 - val_correlation_coefficient: 0.2832\n",
      "Epoch 44/50\n",
      "15065/15065 [==============================] - 11s 750us/step - loss: 0.0182 - correlation_coefficient: 0.9323 - val_loss: 0.1751 - val_correlation_coefficient: 0.2771\n",
      "Epoch 45/50\n",
      "15065/15065 [==============================] - 11s 697us/step - loss: 0.0185 - correlation_coefficient: 0.9312 - val_loss: 0.1744 - val_correlation_coefficient: 0.2872\n",
      "Epoch 46/50\n",
      "15065/15065 [==============================] - 11s 753us/step - loss: 0.0193 - correlation_coefficient: 0.9281 - val_loss: 0.1733 - val_correlation_coefficient: 0.2954\n",
      "Epoch 47/50\n",
      "15065/15065 [==============================] - 11s 744us/step - loss: 0.0190 - correlation_coefficient: 0.9292 - val_loss: 0.1778 - val_correlation_coefficient: 0.2656\n",
      "Epoch 48/50\n",
      "15065/15065 [==============================] - 11s 748us/step - loss: 0.0179 - correlation_coefficient: 0.9333 - val_loss: 0.1814 - val_correlation_coefficient: 0.2538\n",
      "Epoch 49/50\n",
      " 7458/15065 [=============>................] - ETA: 6s - loss: 0.0160 - correlation_coefficient: 0.9406"
     ]
    }
   ],
   "source": [
    "model.fit([xA_train,xB_train], y_train,\n",
    "          batch_size=339,\n",
    "          epochs=50,validation_data=([xA_val,xB_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate([xA_test,xB_test], y_test, batch_size=339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sent_A = get_tokenized_padded_data(np.array(['There is no boy playing outdoors and there is no man smiling']))\n",
    "# sent_B = get_tokenized_padded_data(np.array(['A group of kids is playing in a yard and an old man is standing in the background']))\n",
    "\n",
    "sent_A = get_tokenized_padded_data(np.array(['The cat sits on the mat']))\n",
    "sent_B = get_tokenized_padded_data(np.array(['This models predicts semantic similarity']))\n",
    "\n",
    "# sent_A = np.reshape(sent_A, [1])\n",
    "# sent_B = np.reshape(sent_B, [-1,1])\n",
    "pred = model.predict([sent_A, sent_B])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt;\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.close()\n",
    "\n",
    "y = pred[0]\n",
    "N = len(y)\n",
    "x = [0,1,2,3,4,5]\n",
    "width = 1\n",
    "plt.bar(x, y, width, color=\"blue\")\n",
    "\n",
    "# plt.xticks(x, y)\n",
    "plt.ylabel('Probability distribution of similarity score')\n",
    "plt.title('Prediction of simila')\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
