{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "vecmodel = KeyedVectors.load_word2vec_format('/home/raja/models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Lambda, Dropout, Dot\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, MaxPooling2D,Convolution2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras import backend as K\n",
    "from scipy.stats.stats import pearsonr   \n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "import re\n",
    "import string\n",
    "from numpy.core.umath_tests import inner1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entailment_judgment</th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTAILMENT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entailment_judgment  pair_ID  relatedness_score  \\\n",
       "0             NEUTRAL      1.0                4.5   \n",
       "1             NEUTRAL      2.0                3.2   \n",
       "2          ENTAILMENT      3.0                4.7   \n",
       "3             NEUTRAL      5.0                3.4   \n",
       "4             NEUTRAL      9.0                3.7   \n",
       "\n",
       "                                          sentence_A  \\\n",
       "0  A group of kids is playing in a yard and an ol...   \n",
       "1  A group of children is playing in the house an...   \n",
       "2  The young boys are playing outdoors and the ma...   \n",
       "3  The kids are playing outdoors near a man with ...   \n",
       "4  The young boys are playing outdoors and the ma...   \n",
       "\n",
       "                                          sentence_B  \n",
       "0  A group of boys in a yard is playing and a man...  \n",
       "1  A group of kids is playing in a yard and an ol...  \n",
       "2  The kids are playing outdoors near a man with ...  \n",
       "3  A group of kids is playing in a yard and an ol...  \n",
       "4  A group of kids is playing in a yard and an ol...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = \"['pair_ID', 'sentence_A', 'sentence_B', 'relatedness_score','entailment_judgment']\"\n",
    "train_df = pd.read_csv(\"../data/enhance_traindata.csv\", sep='\\t')\n",
    "trial_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_trial.txt\", sep='\\t')\n",
    "test_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_test_annotated.txt\", sep='\\t')\n",
    "\n",
    "texts = []\n",
    "BASE_DIR = ''\n",
    "TEXT_DATA_DIR = os.path.join('../data/SemEval2014_dataset/')\n",
    "MAX_SEQUENCE_LENGTH = 75\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word_id\n",
    "merged = train_df['sentence_A'].tolist()\n",
    "merged.extend(train_df['sentence_B'].tolist())\n",
    "merged.extend(trial_df['sentence_A'].tolist())\n",
    "merged.extend(trial_df['sentence_B'].tolist())\n",
    "merged.extend(test_df['sentence_A'].tolist())\n",
    "merged.extend(test_df['sentence_B'].tolist())\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(merged)\n",
    "sequences = tokenizer.texts_to_sequences(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "# prepare embedding matrix\n",
    "num_words = max(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in vecmodel.vocab:\n",
    "        embedding_vector = vecmodel[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_padded_data(data):\n",
    "    \n",
    "#     t = Tokenizer(num_words=MAX1_NB_WORDS)\n",
    "#     t.fit_on_texts(data)\n",
    "#     r = RegexpTokenizer(r'\\w+')\n",
    "#     r.tokenize(data)\n",
    "    padded_data = []\n",
    "    for d in data:\n",
    "        r = RegexpTokenizer(r'\\w+')\n",
    "        c = r.tokenize(d)\n",
    "        seq = tokenizer.texts_to_sequences(c)\n",
    "        seq = [item for sublist in seq for item in sublist]\n",
    "        padded_data.append(seq)    \n",
    "    padded_data = pad_sequences(padded_data, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "    return padded_data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# get_tokenized_padded_data(['The young \"boys\" @ are playing outdoors and the man is smiling nearby','Nobody is riding the bicycle on one wheel'])\n",
    "def get_target_value(data):\n",
    "    \n",
    "    target= []\n",
    "    for i in range(len(data)):\n",
    "        target.append(int(round(data[i])));\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_target_category(data):\n",
    "    data = get_target_value(data)\n",
    "    encoder = LabelEncoder()\n",
    "    class_val = [0,1,2,3,4,5] \n",
    "    encoder.fit(class_val)\n",
    "    encoded_Y = encoder.transform(data)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    target = keras.utils.to_categorical(encoded_Y)\n",
    "    return target\n",
    "\n",
    "def pear_coef(y_true, y_pred):\n",
    "    pearson_r, update_op = pearsonr(y_pred, y_true)\n",
    "    return pearson_r\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "   \n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = (x - mx), (y-my)\n",
    "    r_num = K.sum(xm * ym)\n",
    "    r_den = K.sqrt(K.sum(K.square(xm))) * K.sqrt(K.sum(K.square(ym)))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   1  87   6 219   3  16   4   1 616   5  25 285  10   3  42\n",
      "   4   2 402]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xA_train = get_tokenized_padded_data(train_df['sentence_A'].tolist())\n",
    "print(xA_train[0])\n",
    "xA_val = get_tokenized_padded_data(trial_df['sentence_A'].tolist())\n",
    "xA_test = get_tokenized_padded_data(test_df['sentence_A'].tolist())\n",
    "\n",
    "xB_train = get_tokenized_padded_data(train_df['sentence_B'].tolist())\n",
    "xB_val = get_tokenized_padded_data(trial_df['sentence_B'].tolist())\n",
    "xB_test = get_tokenized_padded_data(test_df['sentence_B'].tolist())\n",
    "\n",
    "y_train = train_df['relatedness_score'].tolist()\n",
    "y_val = trial_df['relatedness_score'].tolist()\n",
    "y_test = test_df['relatedness_score'].tolist()\n",
    "\n",
    "# y_train = get_target_value(train_df['relatedness_score'].tolist())\n",
    "# y_val = get_target_value(trial_df['relatedness_score'].tolist())\n",
    "# y_test = get_target_value(test_df['relatedness_score'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.tile(np.arange(6),(len(y_train),1))\n",
    "r_val = np.tile(np.arange(6),(len(y_val),1))\n",
    "r_test = np.tile(np.arange(6),(len(y_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5],\n",
       "       ..., \n",
       "       [0, 1, 2, 3, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5],\n",
       "       [0, 1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of kids is playing in a yard and an old man is standing in the background\n",
      "A group of boys in a yard is playing and a man is standing in the background\n",
      "4.5\n",
      "[[0 1 2 3 4 5]\n",
      " [0 1 2 3 4 5]\n",
      " [0 1 2 3 4 5]\n",
      " ..., \n",
      " [0 1 2 3 4 5]\n",
      " [0 1 2 3 4 5]\n",
      " [0 1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "print(train_df['sentence_A'][0])\n",
    "print(train_df['sentence_B'][0])\n",
    "print(y_train[0])\n",
    "print(r_val)\n",
    "# print(y_train.shape)\n",
    "# print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/raja/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/raja/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/raja/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "seqA_input = Input(shape=(MAX_SEQUENCE_LENGTH,),)\n",
    "seqB_input = Input(shape=(MAX_SEQUENCE_LENGTH,),)\n",
    "r = Input(shape=(6,),)\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "x_A = embedding_layer(seqA_input)\n",
    "x_B = embedding_layer(seqB_input)\n",
    "x = keras.layers.Concatenate()([x_A, x_B])\n",
    "reshape = Reshape((2,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(x)\n",
    "x = Convolution2D(filters=EMBEDDING_DIM,kernel_size=(1,1),activation=\"relu\",kernel_initializer='he_uniform')(reshape)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(1,MAX_SEQUENCE_LENGTH),strides=(1,1))(x)\n",
    "\n",
    "x_A = Lambda(lambda x: x[:, 0])(x)\n",
    "x_B = Lambda(lambda x: x[:, 1])(x)\n",
    "\n",
    "diff = keras.layers.Subtract()([x_A, x_B])\n",
    "prod = keras.layers.Multiply()([x_A, x_B])\n",
    "\n",
    "\n",
    "nn = keras.layers.Concatenate()([diff, prod])\n",
    "\n",
    "\n",
    "\n",
    "nn = Dense(300, activation='tanh',kernel_initializer='he_uniform')(nn)\n",
    "nn = GlobalMaxPooling1D()(nn)\n",
    "nn = Dropout(0.2)(nn)\n",
    "preds = Dense(6, activation='softmax')(nn)\n",
    "# final = Dot(axes=1)([preds,r])\n",
    "final = Lambda(lambda preds: Dot(axes=1)([preds,r]))(preds)\n",
    "\n",
    "model = Model(inputs=[seqA_input,seqB_input,r], outputs=final)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt,\n",
    "              metrics=[correlation_coefficient,'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 300)      60000000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 600)      0           embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 75, 300)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 75, 300)   90300       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2, 75, 300)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 2, 1, 300)    0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 300)       0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 300)       0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 1, 300)       0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1, 300)       0           lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 600)       0           subtract_1[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 300)       180300      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1806        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 60,272,406\n",
      "Trainable params: 272,406\n",
      "Non-trainable params: 60,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15065 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "15065/15065 [==============================] - 10s 673us/step - loss: 1.7604 - correlation_coefficient: 0.5706 - acc: 0.0407 - val_loss: 0.9511 - val_correlation_coefficient: 0.2683 - val_acc: 0.0380\n",
      "Epoch 2/30\n",
      "15065/15065 [==============================] - 8s 542us/step - loss: 1.3442 - correlation_coefficient: 0.7043 - acc: 0.0514 - val_loss: 0.9019 - val_correlation_coefficient: 0.3871 - val_acc: 0.0420\n",
      "Epoch 3/30\n",
      "15065/15065 [==============================] - 8s 518us/step - loss: 1.1776 - correlation_coefficient: 0.7481 - acc: 0.0548 - val_loss: 0.9479 - val_correlation_coefficient: 0.4378 - val_acc: 0.0400\n",
      "Epoch 4/30\n",
      "15065/15065 [==============================] - 8s 543us/step - loss: 1.0751 - correlation_coefficient: 0.7756 - acc: 0.0591 - val_loss: 0.8763 - val_correlation_coefficient: 0.3806 - val_acc: 0.0460\n",
      "Epoch 5/30\n",
      "15065/15065 [==============================] - 8s 557us/step - loss: 0.9672 - correlation_coefficient: 0.8004 - acc: 0.0624 - val_loss: 0.7317 - val_correlation_coefficient: 0.5255 - val_acc: 0.0620\n",
      "Epoch 6/30\n",
      "15065/15065 [==============================] - 8s 554us/step - loss: 0.8999 - correlation_coefficient: 0.8180 - acc: 0.0643 - val_loss: 0.7802 - val_correlation_coefficient: 0.5398 - val_acc: 0.0380\n",
      "Epoch 7/30\n",
      "15065/15065 [==============================] - 8s 545us/step - loss: 0.8025 - correlation_coefficient: 0.8361 - acc: 0.0818 - val_loss: 0.7286 - val_correlation_coefficient: 0.5523 - val_acc: 0.0460\n",
      "Epoch 8/30\n",
      "15065/15065 [==============================] - 8s 548us/step - loss: 0.7542 - correlation_coefficient: 0.8467 - acc: 0.1028 - val_loss: 0.8281 - val_correlation_coefficient: 0.5443 - val_acc: 0.0460\n",
      "Epoch 9/30\n",
      "15065/15065 [==============================] - 8s 545us/step - loss: 0.6879 - correlation_coefficient: 0.8606 - acc: 0.1173 - val_loss: 0.8055 - val_correlation_coefficient: 0.5505 - val_acc: 0.0480\n",
      "Epoch 10/30\n",
      "15065/15065 [==============================] - 8s 548us/step - loss: 0.6500 - correlation_coefficient: 0.8690 - acc: 0.1212 - val_loss: 1.1975 - val_correlation_coefficient: 0.5222 - val_acc: 0.0600\n",
      "Epoch 11/30\n",
      "15065/15065 [==============================] - 8s 549us/step - loss: 0.6271 - correlation_coefficient: 0.8744 - acc: 0.1341 - val_loss: 0.8304 - val_correlation_coefficient: 0.5920 - val_acc: 0.0500\n",
      "Epoch 12/30\n",
      "15065/15065 [==============================] - 8s 548us/step - loss: 0.6144 - correlation_coefficient: 0.8783 - acc: 0.1357 - val_loss: 0.9570 - val_correlation_coefficient: 0.5628 - val_acc: 0.0500\n",
      "Epoch 13/30\n",
      "15065/15065 [==============================] - 8s 553us/step - loss: 0.5838 - correlation_coefficient: 0.8837 - acc: 0.1430 - val_loss: 0.7863 - val_correlation_coefficient: 0.5733 - val_acc: 0.0580\n",
      "Epoch 14/30\n",
      "15065/15065 [==============================] - 8s 549us/step - loss: 0.5322 - correlation_coefficient: 0.8934 - acc: 0.1516 - val_loss: 0.6845 - val_correlation_coefficient: 0.6068 - val_acc: 0.0680\n",
      "Epoch 15/30\n",
      "15065/15065 [==============================] - 8s 552us/step - loss: 0.5315 - correlation_coefficient: 0.8941 - acc: 0.1525 - val_loss: 0.8538 - val_correlation_coefficient: 0.6014 - val_acc: 0.0560\n",
      "Epoch 16/30\n",
      "15065/15065 [==============================] - 8s 551us/step - loss: 0.5213 - correlation_coefficient: 0.8971 - acc: 0.1572 - val_loss: 0.6713 - val_correlation_coefficient: 0.6057 - val_acc: 0.0760\n",
      "Epoch 17/30\n",
      "15065/15065 [==============================] - 8s 547us/step - loss: 0.5015 - correlation_coefficient: 0.9014 - acc: 0.1627 - val_loss: 0.5907 - val_correlation_coefficient: 0.6398 - val_acc: 0.0800\n",
      "Epoch 18/30\n",
      "15065/15065 [==============================] - 8s 548us/step - loss: 0.4738 - correlation_coefficient: 0.9053 - acc: 0.1644 - val_loss: 0.6268 - val_correlation_coefficient: 0.6246 - val_acc: 0.0780\n",
      "Epoch 19/30\n",
      "15065/15065 [==============================] - 8s 551us/step - loss: 0.4689 - correlation_coefficient: 0.9077 - acc: 0.1679 - val_loss: 0.6083 - val_correlation_coefficient: 0.6239 - val_acc: 0.0740\n",
      "Epoch 20/30\n",
      "15065/15065 [==============================] - 8s 549us/step - loss: 0.4565 - correlation_coefficient: 0.9105 - acc: 0.1713 - val_loss: 0.6109 - val_correlation_coefficient: 0.6261 - val_acc: 0.0680\n",
      "Epoch 21/30\n",
      "15065/15065 [==============================] - 8s 553us/step - loss: 0.4323 - correlation_coefficient: 0.9149 - acc: 0.1764 - val_loss: 0.7365 - val_correlation_coefficient: 0.6178 - val_acc: 0.0760\n",
      "Epoch 22/30\n",
      "15065/15065 [==============================] - 8s 549us/step - loss: 0.4305 - correlation_coefficient: 0.9155 - acc: 0.1722 - val_loss: 0.7496 - val_correlation_coefficient: 0.6210 - val_acc: 0.0680\n",
      "Epoch 23/30\n",
      "15065/15065 [==============================] - 8s 553us/step - loss: 0.4313 - correlation_coefficient: 0.9163 - acc: 0.1777 - val_loss: 0.6044 - val_correlation_coefficient: 0.6354 - val_acc: 0.0760\n",
      "Epoch 24/30\n",
      "15065/15065 [==============================] - 8s 550us/step - loss: 0.4155 - correlation_coefficient: 0.9192 - acc: 0.1792 - val_loss: 0.6492 - val_correlation_coefficient: 0.6385 - val_acc: 0.0760\n",
      "Epoch 25/30\n",
      "15065/15065 [==============================] - 8s 551us/step - loss: 0.4025 - correlation_coefficient: 0.9214 - acc: 0.1835 - val_loss: 0.7130 - val_correlation_coefficient: 0.6180 - val_acc: 0.0680\n",
      "Epoch 26/30\n",
      "15065/15065 [==============================] - 8s 549us/step - loss: 0.3881 - correlation_coefficient: 0.9241 - acc: 0.1881 - val_loss: 0.7109 - val_correlation_coefficient: 0.6187 - val_acc: 0.0620\n",
      "Epoch 27/30\n",
      "15065/15065 [==============================] - 8s 558us/step - loss: 0.3753 - correlation_coefficient: 0.9264 - acc: 0.1916 - val_loss: 0.7641 - val_correlation_coefficient: 0.6140 - val_acc: 0.0680\n",
      "Epoch 28/30\n",
      "15065/15065 [==============================] - 8s 554us/step - loss: 0.3906 - correlation_coefficient: 0.9236 - acc: 0.1889 - val_loss: 0.8736 - val_correlation_coefficient: 0.6272 - val_acc: 0.0720\n",
      "Epoch 29/30\n",
      "15065/15065 [==============================] - 8s 548us/step - loss: 0.3860 - correlation_coefficient: 0.9253 - acc: 0.1887 - val_loss: 0.6421 - val_correlation_coefficient: 0.6333 - val_acc: 0.0700\n",
      "Epoch 30/30\n",
      "15065/15065 [==============================] - 8s 551us/step - loss: 0.3680 - correlation_coefficient: 0.9283 - acc: 0.1904 - val_loss: 0.6474 - val_correlation_coefficient: 0.6461 - val_acc: 0.0760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9eaefc898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([xA_train,xB_train, r_train], y_train,\n",
    "          batch_size=100,\n",
    "          epochs=30,validation_data=([xA_val,xB_val,r_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927/4927 [==============================] - 1s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60219708460084265, 0.61432085102316281, 0.065151208295271948]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([xA_test,xB_test,r_test], y_test, batch_size=339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
