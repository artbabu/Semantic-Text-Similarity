{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "vecmodel = KeyedVectors.load_word2vec_format('/home/raja/models/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Lambda, Dropout, Dot\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Conv2D, MaxPooling2D,Convolution2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras import backend as K\n",
    "from scipy.stats.stats import pearsonr   \n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "import re\n",
    "import string\n",
    "from numpy.core.umath_tests import inner1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = \"['pair_ID', 'sentence_A', 'sentence_B', 'relatedness_score','entailment_judgment']\"\n",
    "train_df = pd.read_csv(\"../data/enhance_traindata.csv\", sep='\\t')\n",
    "trial_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_trial.txt\", sep='\\t')\n",
    "test_df = pd.read_csv(\"../data/SemEval2014_dataset/SICK_test_annotated.txt\", sep='\\t')\n",
    "\n",
    "texts = []\n",
    "BASE_DIR = ''\n",
    "TEXT_DATA_DIR = os.path.join('../data/SemEval2014_dataset/')\n",
    "MAX_SEQUENCE_LENGTH = 75\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word_id\n",
    "merged = train_df['sentence_A'].tolist()\n",
    "merged.extend(train_df['sentence_B'].tolist())\n",
    "merged.extend(trial_df['sentence_A'].tolist())\n",
    "merged.extend(trial_df['sentence_B'].tolist())\n",
    "merged.extend(test_df['sentence_A'].tolist())\n",
    "merged.extend(test_df['sentence_B'].tolist())\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(merged)\n",
    "sequences = tokenizer.texts_to_sequences(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "# prepare embedding matrix\n",
    "num_words = max(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in vecmodel.vocab:\n",
    "        embedding_vector = vecmodel[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_padded_data(data):\n",
    "    \n",
    "#     t = Tokenizer(num_words=MAX1_NB_WORDS)\n",
    "#     t.fit_on_texts(data)\n",
    "#     r = RegexpTokenizer(r'\\w+')\n",
    "#     r.tokenize(data)\n",
    "    padded_data = []\n",
    "    for d in data:\n",
    "        r = RegexpTokenizer(r'\\w+')\n",
    "        c = r.tokenize(d)\n",
    "        seq = tokenizer.texts_to_sequences(c)\n",
    "        seq = [item for sublist in seq for item in sublist]\n",
    "        padded_data.append(seq)    \n",
    "    padded_data = pad_sequences(padded_data, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "    return padded_data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# get_tokenized_padded_data(['The young \"boys\" @ are playing outdoors and the man is smiling nearby','Nobody is riding the bicycle on one wheel'])\n",
    "def get_target_value(data):\n",
    "    \n",
    "    target= []\n",
    "    for i in range(len(data)):\n",
    "        target.append(int(round(data[i])));\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_target_category(data):\n",
    "    data = get_target_value(data)\n",
    "    encoder = LabelEncoder()\n",
    "    class_val = [0,1,2,3,4,5] \n",
    "    encoder.fit(class_val)\n",
    "    encoded_Y = encoder.transform(data)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    target = keras.utils.to_categorical(encoded_Y)\n",
    "    return target\n",
    "\n",
    "def pear_coef(y_true, y_pred):\n",
    "    pearson_r, update_op = pearsonr(y_pred, y_true)\n",
    "    return pearson_r\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "   \n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def correlation_coefficient(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = (x - mx), (y-my)\n",
    "    r_num = K.sum(xm * ym)\n",
    "    r_den = K.sqrt(K.sum(K.square(xm))) * K.sqrt(K.sum(K.square(ym)))\n",
    "    r = r_num / r_den\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xA_train = get_tokenized_padded_data(train_df['sentence_A'].tolist())\n",
    "print(xA_train[0])\n",
    "xA_val = get_tokenized_padded_data(trial_df['sentence_A'].tolist())\n",
    "xA_test = get_tokenized_padded_data(test_df['sentence_A'].tolist())\n",
    "\n",
    "xB_train = get_tokenized_padded_data(train_df['sentence_B'].tolist())\n",
    "xB_val = get_tokenized_padded_data(trial_df['sentence_B'].tolist())\n",
    "xB_test = get_tokenized_padded_data(test_df['sentence_B'].tolist())\n",
    "\n",
    "y_train = train_df['relatedness_score'].tolist()\n",
    "y_val = trial_df['relatedness_score'].tolist()\n",
    "y_test = test_df['relatedness_score'].tolist()\n",
    "\n",
    "# y_train = get_target_value(train_df['relatedness_score'].tolist())\n",
    "# y_val = get_target_value(trial_df['relatedness_score'].tolist())\n",
    "# y_test = get_target_value(test_df['relatedness_score'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = np.tile(np.arange(6),(len(y_train),1))\n",
    "r_val = np.tile(np.arange(6),(len(y_val),1))\n",
    "r_test = np.tile(np.arange(6),(len(y_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['sentence_A'][0])\n",
    "print(train_df['sentence_B'][0])\n",
    "print(y_train[0])\n",
    "print(r_val)\n",
    "# print(y_train.shape)\n",
    "# print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqA_input = Input(shape=(MAX_SEQUENCE_LENGTH,),)\n",
    "seqB_input = Input(shape=(MAX_SEQUENCE_LENGTH,),)\n",
    "r = Input(shape=(6,),)\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "x_A = embedding_layer(seqA_input)\n",
    "x_B = embedding_layer(seqB_input)\n",
    "x = keras.layers.Concatenate()([x_A, x_B])\n",
    "reshape = Reshape((2,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(x)\n",
    "x = Convolution2D(filters=EMBEDDING_DIM,kernel_size=(1,1),activation=\"relu\",kernel_initializer='he_uniform')(reshape)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(1,MAX_SEQUENCE_LENGTH),strides=(1,1))(x)\n",
    "\n",
    "x_A = Lambda(lambda x: x[:, 0])(x)\n",
    "x_B = Lambda(lambda x: x[:, 1])(x)\n",
    "\n",
    "diff = keras.layers.Subtract()([x_A, x_B])\n",
    "prod = keras.layers.Multiply()([x_A, x_B])\n",
    "\n",
    "\n",
    "nn = keras.layers.Concatenate()([diff, prod])\n",
    "\n",
    "\n",
    "\n",
    "nn = Dense(300, activation='tanh',kernel_initializer='he_uniform')(nn)\n",
    "nn = GlobalMaxPooling1D()(nn)\n",
    "nn = Dropout(0.2)(nn)\n",
    "preds = Dense(6, activation='softmax')(nn)\n",
    "# final = Dot(axes=1)([preds,r])\n",
    "final = Lambda(lambda preds: Dot(axes=1)([preds,r]))(preds)\n",
    "\n",
    "model = Model(inputs=[seqA_input,seqB_input,r], outputs=final)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt,\n",
    "              metrics=[correlation_coefficient,'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([xA_train,xB_train, r_train], y_train,\n",
    "          batch_size=100,\n",
    "          epochs=30,validation_data=([xA_val,xB_val,r_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([xA_test,xB_test,r_test], y_test, batch_size=339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
