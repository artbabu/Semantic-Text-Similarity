{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import *\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "    print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import itertools\n",
    "\n",
    "def flatten(tokens):\n",
    "    tokens2 = [([x] if isinstance(x,str) else x) for x in tokens]\n",
    "    flattened = list(itertools.chain(*tokens2))\n",
    "    return flattened\n",
    "\n",
    "# def get_wordnet_pos(tag):\n",
    "#     if tag.startswith('J'):\n",
    "#         return wn.ADJ\n",
    "#     elif tag.startswith('V'):\n",
    "#         return wn.VERB\n",
    "#     elif tag.startswith('N'):\n",
    "#         return wn.NOUN\n",
    "#     elif tag.startswith('R'):\n",
    "#         return wn.ADV\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# def tag_pos (sentence):\n",
    "#     tagged = pos_tag(sentence)\n",
    "#     return tagged\n",
    "\n",
    "# def replace_base (sentence):\n",
    "#     tagged = tag_pos(sentence)\n",
    "#     replaced = []\n",
    "#     for i in range(len(tagged)):\n",
    "#         token = tagged[i]\n",
    "#         word = token[0]\n",
    "#         pos = get_wordnet_pos(token[1])\n",
    "#         base = wn.morphy(word, pos)\n",
    "#         if(base != None):\n",
    "#             replaced.append(base)\n",
    "#         else:\n",
    "#             replaced.append(word)\n",
    "#     return replaced\n",
    "\n",
    "def synonyms_word (word):\n",
    "    wordlemma = []\n",
    "    try:\n",
    "        synsets = wn.synsets(word)\n",
    "        for synset in synsets:\n",
    "            lemma = [str(lemma.name()) for lemma in synset.lemmas()]\n",
    "            wordlemma.append(lemma)\n",
    "        flattened = flatten(wordlemma)\n",
    "        unique = list(set(flattened))\n",
    "        return unique\n",
    "    except:\n",
    "        print word\n",
    "        return []\n",
    "\n",
    "def synonyms_sentence (sentence):\n",
    "    syn = []\n",
    "    for word in sentence:\n",
    "        syn.append(synonyms_word(word))\n",
    "        \n",
    "    return syn\n",
    "\n",
    "def replace_syn(sentence1, sentence2):\n",
    "    updated_sentence = []\n",
    "    \n",
    "    for i in range(len(sentence2)):\n",
    "        a = set(synonyms_word(sentence2[i]))\n",
    "        for replacew in sentence1:\n",
    "            b = set(synonyms_word(replacew))\n",
    "            if(not set(a).isdisjoint(b)):\n",
    "                sentence2[i] = replacew\n",
    "                break\n",
    "    return sentence1,sentence2\n",
    "\n",
    "# s1 = word_tokenize('The bird is bathing in the sink.')\n",
    "# s2 = word_tokenize('Birdie is washing itself in the water basin.')\n",
    "\n",
    "# u1,u2 = replace_syn(s1,s2)\n",
    "# print u1\n",
    "# print u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ngram_word_overlap(s1,s2,ngram):\n",
    "    \n",
    "    tokenised_s1, tokenised_s2 = replace_syn(word_tokenize(s1),word_tokenize(s2))\n",
    "\n",
    "    ngrams_list_s1 = set(ngrams(tokenised_s1, ngram))\n",
    "    ngrams_list_s2 = set(ngrams(tokenised_s2, ngram))\n",
    "    \n",
    "    s1_s2_intersection = list(ngrams_list_s1 & ngrams_list_s2)\n",
    "    \n",
    "#     print(s1_s2_intersection)\n",
    "    \n",
    "    word_overlap = (2 * (np.divide(len(ngrams_list_s1), len(s1_s2_intersection)) +\n",
    "                        np.divide(len(ngrams_list_s2), len(s1_s2_intersection)))) - 1\n",
    "    \n",
    "    return word_overlap\n",
    "\n",
    "def get_ngram_char_overlap(s1,s2,ngram):\n",
    "    \n",
    "    ngrams_list_s1 = set([\"\".join(j) for j in zip(*[s1[i:] for i in range(ngram)])])\n",
    "    ngrams_list_s2 = set([\"\".join(j) for j in zip(*[s2[i:] for i in range(ngram)])])\n",
    "    \n",
    "    s1_s2_intersection = list(ngrams_list_s1 & ngrams_list_s2)\n",
    "    \n",
    "#     print(s1_s2_intersection)\n",
    "    \n",
    "    char_overlap = (2 * (np.divide(len(ngrams_list_s1), len(s1_s2_intersection)) +\n",
    "                        np.divide(len(ngrams_list_s2), len(s1_s2_intersection)))) - 1\n",
    "    return char_overlap\n",
    "\n",
    "\n",
    "\n",
    "s1 = 'The villains I absolutely hate are selfish and self-serving, but they are also cowards.'\n",
    "s2 = 'A villain you want to take down is, at his/her core, someone who does not care about the suffering of others.'\n",
    "n = 1\n",
    "\n",
    "get_ngram_word_overlap(s1, s2, n)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longestCommonPrefix(strs):\n",
    "        \n",
    "        if not strs:\n",
    "            return \"\"\n",
    "\n",
    "        for i in xrange(len(strs[0])):\n",
    "            for string in strs[1:]:\n",
    "                if i >= len(string) or string[i] != strs[0][i]:\n",
    "                    return strs[0][:i]\n",
    "        return len(strs[0])\n",
    "    \n",
    "def longestCommonSuffix(strs):\n",
    "        \n",
    "        if not strs:\n",
    "            return \"\"\n",
    "        s1_len = len(strs[0])\n",
    "        s2_len = len(strs[1])\n",
    "        while s1_len > 0 and s2_len > 0:\n",
    "            if strs[1][s2_len - 1] != strs[0][s1_len - 1]:\n",
    "                return strs[0][s1_len - 1:-1]\n",
    "            s1_len -= 1; \n",
    "            s2_len -= 1;\n",
    "        return len(strs[0])\n",
    "    \n",
    "def longestSubstringFinder(s1, s2):\n",
    "    answer = \"\"\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    for i in range(len1):\n",
    "        match = \"\"\n",
    "        for j in range(len2):\n",
    "            if (i + j < len1 and s1[i + j] == s2[j]):\n",
    "                match += s2[j]\n",
    "            else:\n",
    "                if (len(match) > len(answer)): answer = match\n",
    "                match = \"\"\n",
    "    return len(answer)\n",
    "\n",
    "def get_levenshtein_distance(s1, s2):\n",
    "    return edit_distance(word_tokenize(s1),word_tokenize(s2))\n",
    "\n",
    "\n",
    "def get_binary_distance(s1, s2):\n",
    "    return binary_distance(word_tokenize(s1),word_tokenize(s2))\n",
    "\n",
    "# def get_jaccard_distance(s1, s2):\n",
    "#     return jaccard_distance(word_tokenize(s1),word_tokenize(s2))\n",
    "\n",
    "# def get_masi_distance(s1, s2):\n",
    "#     return masi_distance(set(word_tokenize(s1)),set(word_tokenize(s2)))\n",
    "\n",
    "# def get_interval_distance(s1, s2):\n",
    "#     return interval_distance(word_tokenize(s1),word_tokenize(s2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# s1 = word_tokenize('The bird is bathing in the sink.')\n",
    "# s2 = word_tokenize('Birdie is washing itself in the water basin.')\n",
    "# # print longestCommonPrefix([ \"heaven\", \"heavy\", \"heallo\"])   \n",
    "# get_masi_distance(s1,s2)\n",
    "# # get_jaccard_distance('The bird is bathing in the sink.','Birdie is washing itself in the water basin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../../../GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
    "\n",
    "def convert2BOWV(word_list):\n",
    "    wv_list = []\n",
    "    \n",
    "    for w in word_list:\n",
    "        if w in model.vocab:\n",
    "            wv_list.append(model[w]) \n",
    "    return wv_list\n",
    "\n",
    "# sum of all word vectors\n",
    "def getSentenceEmbedings(word_list):\n",
    "    wv_list = convert2BOWV(word_list)\n",
    "    sv = wv_list[0]\n",
    "    for i in range(len(wv_list)):\n",
    "        if i != 0:\n",
    "            sv = np.add(sv,wv_list[i])\n",
    "    return sv.reshape(1, -1)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-71-5300b793c2b0>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-5300b793c2b0>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    if !alignment:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('I'):\n",
    "        return 'p'\n",
    "    elif tag.startswith('CD'):\n",
    "        return 'cd'\n",
    "    else:\n",
    "        return None\n",
    "def change_tag_to_WNpos(sentence):\n",
    "    tagged_sent = []\n",
    "    for w, tag in sentence:\n",
    "        tagged_sent.append([w,get_wordnet_pos(tag)])\n",
    "    return tagged_sent    \n",
    "    \n",
    "def get_alignment_score(s1,s2, alignment, pos=None):\n",
    "    \n",
    "    if alignment == None:\n",
    "        print s1, s2\n",
    "        return\n",
    "    if pos is None:\n",
    "        s1_cnt = len(set(s1))\n",
    "        s2_cnt = len(set(s2))\n",
    "        sa1_cnt = 0\n",
    "        sa2_cnt = 0\n",
    "        for a in alignment:\n",
    "            print a\n",
    "            sa1_cnt += len(s1[a[0] - 1])\n",
    "            sa2_cnt += len(s2[a[1] - 1])\n",
    "        align_score = np.divide(float(sa1_cnt + sa2_cnt),float(s1_cnt + s2_cnt))\n",
    "        return align_score\n",
    "    else:\n",
    "        \n",
    "        s1_pos = change_tag_to_WNpos(nltk.pos_tag(s1))\n",
    "        s2_pos = change_tag_to_WNpos(nltk.pos_tag(s2))\n",
    "        \n",
    "        pos_tag_s1 = sum([1 for c,s in enumerate(s1) if s1_pos[c - 1][1] == pos])\n",
    "        pos_tag_s2 = sum([1 for c,s in enumerate(s2) if s2_pos[c - 1][1] == pos])\n",
    "        \n",
    "        sa1_cnt = 0\n",
    "        sa2_cnt = 0 \n",
    "        \n",
    "        for c,a in enumerate(alignment): \n",
    "            if s1_pos[a[0] - 1][1] == pos and s2_pos[a[1] - 1][1] == pos :\n",
    "                print a\n",
    "                print s1_pos[a[0] - 1]\n",
    "                print s2_pos[a[1] - 1]\n",
    "                sa1_cnt += len(s1[a[0] - 1][0])\n",
    "                sa2_cnt += len(s2[a[1] - 1][0])\n",
    "                print (sa1_cnt + sa2_cnt) /(pos_tag_s2 + pos_tag_s2)\n",
    "        align_score = np.divide(float(sa1_cnt + sa2_cnt),float(pos_tag_s1 + pos_tag_s2))\n",
    "        \n",
    "        return align_score\n",
    "    \n",
    "    \n",
    "    \n",
    "alignment = [[7, 8], [2, 2], [3, 4], [1, 1], [5, 6]]\n",
    "test_align = [['.', '.'], ['men', 'people'], ['died', 'dead'], ['Four', '4'], ['accident', 'collision'], ['an', 'a']]\n",
    "s1 = word_tokenize(\"Four men died in an accident.\")\n",
    "s2 = word_tokenize(\"4 people are dead from a collision.\")\n",
    "\n",
    "# get_alignment_score(s1,s2,alignment,)\n",
    "\n",
    "print change_tag_to_WNpos(nltk.pos_tag(s1))\n",
    "print change_tag_to_WNpos(nltk.pos_tag(s2))\n",
    "print nltk.pos_tag(s1)\n",
    "print nltk.pos_tag(s2)\n",
    "print get_alignment_score(s1,s2,alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.parse.stanford import GenericStanfordParser\n",
    "\n",
    "# g = GenericStanfordParser()\n",
    "# print g.tagged_parse(nltk.pos_tag('Four men died in an accident.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "def encode(s):\n",
    "    return s.encode('utf-8')\n",
    "df = pd.read_pickle(\"ML_Train_Data\")\n",
    "# df['Sentence1'] = df.apply(lambda x: encode(x['Sentence1']), axis=1)\n",
    "# df['Sentence2'] = df.apply(lambda x: encode(x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"1gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],2), axis=1)\n",
    "df[\"2gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],2), axis=1)\n",
    "df[\"3gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],3), axis=1)\n",
    "\n",
    "df[\"2gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],2), axis=1)\n",
    "df[\"3gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],3), axis=1)\n",
    "df[\"4gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],4), axis=1)\n",
    "df[\"5gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"longestCommonPrefix\"] = df.apply(lambda x: longestCommonPrefix([x['Sentence1'], x['Sentence2']]), axis=1)\n",
    "df[\"longestCommonSuffix\"] = df.apply(lambda x: longestCommonSuffix([x['Sentence1'], x['Sentence2']]), axis=1)\n",
    "df[\"longestSubstringFinder\"] = df.apply(lambda x: longestSubstringFinder(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"levenshtein_distance\"] = df.apply(lambda x: get_levenshtein_distance(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "df[\"binary_distance\"] = df.apply(lambda x: get_binary_distance(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"jaccard_distance\"] = df.apply(lambda x: get_jaccard_distance(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "# df[\"masi_distance\"] = df.apply(lambda x: masi_distance(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "# df[\"interval_distance\"] = df.apply(lambda x: get_interval_distance(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentence1</th>\n",
       "      <th>Sentence2</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>1gram_woverlap</th>\n",
       "      <th>2gram_woverlap</th>\n",
       "      <th>3gram_woverlap</th>\n",
       "      <th>2gram_coverlap</th>\n",
       "      <th>3gram_coverlap</th>\n",
       "      <th>4gram_coverlap</th>\n",
       "      <th>5gram_coverlap</th>\n",
       "      <th>longestCommonPrefix</th>\n",
       "      <th>longestCommonSuffix</th>\n",
       "      <th>longestSubstringFinder</th>\n",
       "      <th>levenshtein_distance</th>\n",
       "      <th>binary_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000</td>\n",
       "      <td>render one language in another language</td>\n",
       "      <td>restate (words) from one language into another...</td>\n",
       "      <td>[[[2, 6], [3, 7], [5, 9], [6, 10]], [[one, one...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>re</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.250</td>\n",
       "      <td>nations unified by shared interests, history o...</td>\n",
       "      <td>a group of nations having common interests.</td>\n",
       "      <td>[[[5, 7], [4, 6], [1, 4]], [[interests, intere...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.250</td>\n",
       "      <td>convert into absorbable substances, (as if) wi...</td>\n",
       "      <td>soften or disintegrate by means of chemical ac...</td>\n",
       "      <td>[[[11, 10], [13, 7]], [[heat, heat], [chemical...</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000</td>\n",
       "      <td>devote or adapt exclusively to an skill, study...</td>\n",
       "      <td>devote oneself to a special area of work.</td>\n",
       "      <td>[[[1, 1], [12, 8]], [[devote, devote], [work, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>devote o</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.250</td>\n",
       "      <td>elevated wooden porch of a house</td>\n",
       "      <td>a porch that resembles the deck on a ship.</td>\n",
       "      <td>[[[3, 2]], [[porch, porch]]]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.000</td>\n",
       "      <td>either half of an archery bow</td>\n",
       "      <td>either of the two halves of a bow from handle ...</td>\n",
       "      <td>[[[6, 8], [2, 5], [4, 7], [1, 1], [3, 6]], [[b...</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>either</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.333</td>\n",
       "      <td>a removable device that is an accessory to lar...</td>\n",
       "      <td>a supplementary part or accessory.</td>\n",
       "      <td>[[[7, 5]], [[accessory, accessory]]]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.750</td>\n",
       "      <td>restrict or confine</td>\n",
       "      <td>place limits on (extent or access).</td>\n",
       "      <td>[[[1, 2]], [[restrict, limits]]]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>91</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.500</td>\n",
       "      <td>orient, be positioned</td>\n",
       "      <td>be opposite.</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.750</td>\n",
       "      <td>Bring back to life, return from the dead</td>\n",
       "      <td>cause to become alive again.</td>\n",
       "      <td>[[[1, 1]], [[Bring, cause]]]</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>123</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                          Sentence1  \\\n",
       "0  5.000            render one language in another language   \n",
       "1  3.250  nations unified by shared interests, history o...   \n",
       "2  3.250  convert into absorbable substances, (as if) wi...   \n",
       "3  4.000  devote or adapt exclusively to an skill, study...   \n",
       "4  3.250                   elevated wooden porch of a house   \n",
       "5  4.000                      either half of an archery bow   \n",
       "6  3.333  a removable device that is an accessory to lar...   \n",
       "7  4.750                                restrict or confine   \n",
       "8  0.500                              orient, be positioned   \n",
       "9  4.750           Bring back to life, return from the dead   \n",
       "\n",
       "                                           Sentence2  \\\n",
       "0  restate (words) from one language into another...   \n",
       "1        a group of nations having common interests.   \n",
       "2  soften or disintegrate by means of chemical ac...   \n",
       "3          devote oneself to a special area of work.   \n",
       "4         a porch that resembles the deck on a ship.   \n",
       "5  either of the two halves of a bow from handle ...   \n",
       "6                 a supplementary part or accessory.   \n",
       "7                place limits on (extent or access).   \n",
       "8                                       be opposite.   \n",
       "9                       cause to become alive again.   \n",
       "\n",
       "                                           Alignment  1gram_woverlap  \\\n",
       "0  [[[2, 6], [3, 7], [5, 9], [6, 10]], [[one, one...              13   \n",
       "1  [[[5, 7], [4, 6], [1, 4]], [[interests, intere...              -1   \n",
       "2  [[[11, 10], [13, 7]], [[heat, heat], [chemical...              51   \n",
       "3  [[[1, 1], [12, 8]], [[devote, devote], [work, ...              -1   \n",
       "4                       [[[3, 2]], [[porch, porch]]]              -1   \n",
       "5  [[[6, 8], [2, 5], [4, 7], [1, 1], [3, 6]], [[b...              33   \n",
       "6               [[[7, 5]], [[accessory, accessory]]]              -1   \n",
       "7                   [[[1, 2]], [[restrict, limits]]]              -1   \n",
       "8                                           [[], []]              -1   \n",
       "9                       [[[1, 1]], [[Bring, cause]]]              -1   \n",
       "\n",
       "   2gram_woverlap  3gram_woverlap  2gram_coverlap  3gram_coverlap  \\\n",
       "0              13              -1               5               5   \n",
       "1              -1              -1               7               9   \n",
       "2              51              -1               7              17   \n",
       "3              -1              -1               7              13   \n",
       "4              -1              -1               7              21   \n",
       "5              33              -1               5               9   \n",
       "6              -1              -1               7              17   \n",
       "7              -1              -1              15              31   \n",
       "8              -1              -1               7              11   \n",
       "9              -1              -1              13              61   \n",
       "\n",
       "   4gram_coverlap  5gram_coverlap longestCommonPrefix longestCommonSuffix  \\\n",
       "0               5               5                  re                       \n",
       "1              13              15                                           \n",
       "2              25              35                                           \n",
       "3              17              27            devote o                       \n",
       "4              31              41                                           \n",
       "5              17              35             either                        \n",
       "6              21              25                  a                        \n",
       "7              47              91                                           \n",
       "8              25              49                                           \n",
       "9             123              -1                                           \n",
       "\n",
       "   longestSubstringFinder  levenshtein_distance  binary_distance  \n",
       "0                       9                     7              1.0  \n",
       "1                       5                     9              1.0  \n",
       "2                      10                    13              1.0  \n",
       "3                       8                    10              1.0  \n",
       "4                       7                     9              1.0  \n",
       "5                       7                    11              1.0  \n",
       "6                      10                     8              1.0  \n",
       "7                       2                     8              1.0  \n",
       "8                       5                     4              1.0  \n",
       "9                       4                     8              1.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sv_A'] = df['Sentence1'].map(getSentenceEmbedings)\n",
    "df['sv_B'] = df['Sentence2'].map(getSentenceEmbedings)\t\t\n",
    "df['cos_sim'] = df.apply(lambda x: cosine_similarity(x['sv_A'], x['sv_B']).item(0), axis=1)\n",
    "\n",
    "#Drop temp cols\n",
    "dropCols = ['sv_A', 'sv_B']\n",
    "for d in dropCols:\n",
    "    df.drop(d, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_align(x):\n",
    "    try:\n",
    "        return x['Alignment'].item(0)\n",
    "    except:\n",
    "        print x['Alignment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"'NoneType' object is not iterable\", u'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-e993b36cb418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'N_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4260\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4357\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4358\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4359\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4360\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-e993b36cb418>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'N_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R_align_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_alignment_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alignment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-0615a6ace083>\u001b[0m in \u001b[0;36mget_alignment_score\u001b[0;34m(s1, s2, alignment, pos)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msa1_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msa2_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malignment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msa1_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: (\"'NoneType' object is not iterable\", u'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "df['Alignment'] = df.apply(lambda x: get_align(x), axis=1)\n",
    "df['align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment']), axis=1)\n",
    "df['N_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'n'), axis=1)\n",
    "df['V_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'v'), axis=1)\n",
    "df['R_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'r'), axis=1)\n",
    "df['A_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'a'), axis=1)\n",
    "df['P_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'p'), axis=1)\n",
    "df['CD_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'cd'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[5, 4], [1, 1]], [['concrete', 'concrete'], ['anything', 'something']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Bleu_score'] = df.apply(lambda x:  nltk.translate.bleu_score.sentence_bleu(x['Sentence1'],x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"train_data_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 , s2 = replace_syn(word_tokenize(\"Four men died in an accident.\"), word_tokenize(\"4 people are dead from a collision.\"))\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([s1], s2)\n",
    "print BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
