{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import *\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "    print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ngram_word_overlap(s1,s2,ngram):\n",
    "\n",
    "\n",
    "    ngrams_list_s1 = set(ngrams(s1, ngram))\n",
    "    ngrams_list_s2 = set(ngrams(s2, ngram))\n",
    "    \n",
    "    s1_s2_intersection = list(ngrams_list_s1 & ngrams_list_s2)\n",
    "    \n",
    "#     print(s1_s2_intersection)\n",
    "    \n",
    "    word_overlap = (2 * (np.divide(len(ngrams_list_s1), len(s1_s2_intersection)) +\n",
    "                        np.divide(len(ngrams_list_s2), len(s1_s2_intersection)))) - 1\n",
    "    \n",
    "    return word_overlap\n",
    "\n",
    "def get_ngram_char_overlap(s1,s2,ngram):\n",
    "    \n",
    "    ngrams_list_s1 = set([\"\".join(j) for j in zip(*[s1[i:] for i in range(ngram)])])\n",
    "    ngrams_list_s2 = set([\"\".join(j) for j in zip(*[s2[i:] for i in range(ngram)])])\n",
    "    \n",
    "    s1_s2_intersection = list(ngrams_list_s1 & ngrams_list_s2)\n",
    "    \n",
    "#     print(s1_s2_intersection)\n",
    "    \n",
    "    char_overlap = (2 * (np.divide(len(ngrams_list_s1), len(s1_s2_intersection)) +\n",
    "                        np.divide(len(ngrams_list_s2), len(s1_s2_intersection)))) - 1\n",
    "    return char_overlap\n",
    "\n",
    "\n",
    "\n",
    "s1 = 'The villains I absolutely hate are selfish and self-serving, but they are also cowards.'\n",
    "s2 = 'A villain you want to take down is, at his/her core, someone who does not care about the suffering of others.'\n",
    "n = 1\n",
    "\n",
    "get_ngram_word_overlap(s1, s2, n)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'bird', 'is', 'bathing', 'in', 'the', 'sink', '.']\n",
      "['Birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin', '.']\n",
      "0.692307692308\n"
     ]
    }
   ],
   "source": [
    "def longestCommonPrefix(strs):\n",
    "        \n",
    "        if not strs:\n",
    "            return \"\"\n",
    "        for word_list in strs:\n",
    "            ' '.join(word for word in word_list)\n",
    "        for i in xrange(len(strs[0])):\n",
    "            for string in strs[1:]:\n",
    "                if i >= len(string) or string[i] != strs[0][i]:\n",
    "                    return len(strs[0][:i])\n",
    "        return len(strs[0])\n",
    "    \n",
    "def longestCommonSuffix(strs):\n",
    "        \n",
    "        if not strs:\n",
    "            return \"\"\n",
    "        for word_list in strs:\n",
    "            ' '.join(word for word in word_list)\n",
    "        s1_len = len(strs[0])\n",
    "        s2_len = len(strs[1])\n",
    "        while s1_len > 0 and s2_len > 0:\n",
    "            if strs[1][s2_len - 1] != strs[0][s1_len - 1]:\n",
    "                return len(strs[0][s1_len - 1:-1])\n",
    "            s1_len -= 1; \n",
    "            s2_len -= 1;\n",
    "        return len(strs[0])\n",
    "    \n",
    "def longestSubstringFinder(s1, s2):\n",
    "    answer = \"\"\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    s1 = ' '.join(word for word in s1)\n",
    "    s2 = ' '.join(word for word in s2)\n",
    "    for i in range(len1):\n",
    "        match = \"\"\n",
    "        for j in range(len2):\n",
    "            if (i + j < len1 and s1[i + j] == s2[j]):\n",
    "                match += s2[j]\n",
    "            else:\n",
    "                if (len(match) > len(answer)): answer = match\n",
    "                match = \"\"\n",
    "           \n",
    "    return len(answer)\n",
    "\n",
    "def get_levenshtein_distance(s1, s2):\n",
    "    return edit_distance(s1,s2)\n",
    "\n",
    "\n",
    "def get_binary_distance(s1, s2):\n",
    "    return binary_distance(s1,s2)\n",
    "\n",
    "def get_jaccard_distance(s1, s2):\n",
    "    return jaccard_distance(s1,s2)\n",
    "\n",
    "def get_masi_distance(s1, s2):\n",
    "    return masi_distance(s1,s2)\n",
    "\n",
    "def get_interval_distance(s1, s2):\n",
    "    return interval_distance(s1,s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s1 = word_tokenize('The bird is bathing in the sink.')\n",
    "s2 = word_tokenize('Birdie is washing itself in the water basin.')\n",
    "print s1\n",
    "print s2\n",
    "print get_jaccard_distance(set(s1),set(s2))\n",
    "\n",
    "# get_masi_distance(s1,s2)\n",
    "# # get_jaccard_distance('The bird is bathing in the sink.','Birdie is washing itself in the water basin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../../../../GoogleNews-vectors-negative300.bin.gz', binary=True, limit=600000)\n",
    "\n",
    "def convert2BOWV(word_list):\n",
    "    wv_list = []\n",
    "    \n",
    "    for w in word_list:\n",
    "        if w in model.vocab:\n",
    "            wv_list.append(model[w]) \n",
    "    return wv_list\n",
    "\n",
    "# sum of all word vectors\n",
    "def getSentenceEmbedings(word_list):\n",
    "    \n",
    "    wv_list = convert2BOWV(word_list)\n",
    "    if wv_list:\n",
    "        sv = wv_list[0]\n",
    "        for i in range(len(wv_list)):\n",
    "            if i != 0:\n",
    "                sv = np.add(sv,wv_list[i])\n",
    "        return sv.reshape(1, -1)  \n",
    "    else:\n",
    "        return np.zeros((1,300))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'men', 'died', 'in', 'an', 'accident', '.']\n",
      "['4', 'people', 'are', 'dead', 'from', 'a', 'collision', '.']\n",
      "[['Four', 'cd'], ['men', u'n'], ['died', u'v'], ['in', 'p'], ['an', None], ['accident', u'n'], ['.', None]]\n",
      "[['4', 'cd'], ['people', u'n'], ['are', u'v'], ['dead', u'a'], ['from', 'p'], ['a', None], ['collision', u'n'], ['.', None]]\n",
      "[('Four', 'CD'), ('men', 'NNS'), ('died', 'VBD'), ('in', 'IN'), ('an', 'DT'), ('accident', 'NN'), ('.', '.')]\n",
      "[('4', 'CD'), ('people', 'NNS'), ('are', 'VBP'), ('dead', 'JJ'), ('from', 'IN'), ('a', 'DT'), ('collision', 'NN'), ('.', '.')]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "error_count = 0\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('I'):\n",
    "        return 'p'\n",
    "    elif tag.startswith('CD'):\n",
    "        return 'cd'\n",
    "    else:\n",
    "        return None\n",
    "def change_tag_to_WNpos(sentence):\n",
    "    tagged_sent = []\n",
    "    for w, tag in sentence:\n",
    "        tagged_sent.append([w,get_wordnet_pos(tag)])\n",
    "    return tagged_sent    \n",
    "    \n",
    "def get_alignment_score(s1,s2, alignment, pos=None):\n",
    "    \n",
    "    if alignment == None:\n",
    "        print s1, s2\n",
    "        return\n",
    "    if pos is None:\n",
    "        s1_cnt = len(set(s1))\n",
    "        s2_cnt = len(set(s2))\n",
    "        sa1_cnt = 0\n",
    "        sa2_cnt = 0\n",
    "        for a in alignment:\n",
    "            try:\n",
    "                sa1_cnt += 1\n",
    "                sa2_cnt += 1\n",
    "            except:\n",
    "                print 'errror' \n",
    "                \n",
    "        align_score = np.divide(float(sa1_cnt + sa2_cnt),float(s1_cnt + s2_cnt))\n",
    "        return align_score\n",
    "    else:\n",
    "        \n",
    "        s1_pos = change_tag_to_WNpos(nltk.pos_tag(s1))\n",
    "        s2_pos = change_tag_to_WNpos(nltk.pos_tag(s2))\n",
    "        \n",
    "        pos_tag_s1 = sum([1 for c,s in enumerate(s1) if s1_pos[c][1] == pos])\n",
    "        pos_tag_s2 = sum([1 for c,s in enumerate(s2) if s2_pos[c][1] == pos])\n",
    "                \n",
    "        sa1_cnt = 0\n",
    "        sa2_cnt = 0 \n",
    "        \n",
    "        alignment_map = {}\n",
    "        for c,a in enumerate(alignment):\n",
    "            alignment_map[a[0]] = a[1]\n",
    "        for idx, sent1_pos in enumerate(s1_pos):\n",
    "            try:\n",
    "                if idx in alignment_map and sent1_pos[1] == pos:\n",
    "                    sa1_cnt += 1\n",
    "                    sa2_cnt += 1 \n",
    "            except:\n",
    "                   print 'errror'\n",
    "        align_score = np.divide(float(sa1_cnt + sa2_cnt),float(pos_tag_s1 + pos_tag_s2))\n",
    "        \n",
    "        \n",
    "        return align_score\n",
    "    \n",
    "    \n",
    "    \n",
    "alignment = [[7, 8], [2, 2], [3, 4], [1, 1], [5, 6]]\n",
    "test_align = [['.', '.'], ['men', 'people'], ['died', 'dead'], ['Four', '4'], ['accident', 'collision'], ['an', 'a']]\n",
    "s1 = word_tokenize(\"Four men died in an accident.\")\n",
    "s2 = word_tokenize(\"4 people are dead from a collision.\")\n",
    "\n",
    "# get_alignment_score(s1,s2,alignment,)\n",
    "print s1\n",
    "print s2\n",
    "print change_tag_to_WNpos(nltk.pos_tag(s1))\n",
    "print change_tag_to_WNpos(nltk.pos_tag(s2))\n",
    "print nltk.pos_tag(s1)\n",
    "print nltk.pos_tag(s2)\n",
    "print get_alignment_score(s1,s2,alignment, 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.parse.stanford import GenericStanfordParser\n",
    "\n",
    "# g = GenericStanfordParser()\n",
    "# print g.tagged_parse(nltk.pos_tag('Four men died in an accident.'))\n",
    "# from nltk.tokenize.moses import MosesDetokenizer,MosesTokenizer\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# from nltk.tokenize import PunktWordTokenizer\n",
    "# ptoken = PunktWordTokenizer()\n",
    "# tknzr =MosesTokenizer()\n",
    "# detokenizer = MosesDetokenizer()\n",
    "# def sentence_tokenise(s):\n",
    "#     s = detokenizer.detokenize(s, return_str=True)\n",
    "#     s =  ptoken.tokenize(s)\n",
    "#     s = detokenizer.detokenize(s, return_str=True)\n",
    "#     return tknzr.tokenize(s)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "# def encode(s):\n",
    "#     return s.encode('utf-8')\n",
    "df = pd.read_pickle(\"Preprocessed_ML_Align_TEST.pickle\")\n",
    "df.to_excel(\"Preprocessed_ML_Align_TEST.xlsx\")\n",
    "print \"start\"\n",
    "\n",
    "# df['Sentence1'] = df.apply(lambda x: sentence_tokenise(x['Sentence1']), axis=1)\n",
    "# df['Sentence2'] = df.apply(lambda x: sentence_tokenise(x['Sentence2']), axis=1)\n",
    "# df['Sentence1'] = df.apply(lambda x: encode(x['Sentence1']), axis=1)\n",
    "# df['Sentence2'] = df.apply(lambda x: encode(x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in divide\n",
      "  if sys.path[0] == '':\n",
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in divide\n",
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    }
   ],
   "source": [
    "df[\"1gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],1), axis=1)\n",
    "df[\"2gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],2), axis=1)\n",
    "df[\"3gram_woverlap\"] = df.apply(lambda x: get_ngram_word_overlap(x['Sentence1'], x['Sentence2'],3), axis=1)\n",
    "\n",
    "df[\"2gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],2), axis=1)\n",
    "df[\"3gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],3), axis=1)\n",
    "df[\"4gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],4), axis=1)\n",
    "df[\"5gram_coverlap\"] = df.apply(lambda x: get_ngram_char_overlap(x['Sentence1'], x['Sentence2'],5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"longestCommonPrefix\"] = df.apply(lambda x: longestCommonPrefix([x['Sentence1'], x['Sentence2']]), axis=1)\n",
    "df[\"longestCommonSuffix\"] = df.apply(lambda x: longestCommonSuffix([x['Sentence1'], x['Sentence2']]), axis=1)\n",
    "df[\"longestSubstringFinder\"] = df.apply(lambda x: longestSubstringFinder(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"levenshtein_distance\"] = df.apply(lambda x: get_levenshtein_distance(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "# df[\"binary_distance\"] = df.apply(lambda x: get_binary_distance(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"jaccard_distance\"] = df.apply(lambda x: get_jaccard_distance(set(x['Sentence1']), set(x['Sentence2'])), axis=1)\n",
    "df[\"masi_distance\"] = df.apply(lambda x: masi_distance(set(x['Sentence1']), set(x['Sentence2'])), axis=1)\n",
    "# df[\"interval_distance\"] = df.apply(lambda x: get_interval_distance(set(x['Sentence1']), set(x['Sentence2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df['cos_sim'] = df.apply(lambda x: \n",
    "                         cosine_similarity(getSentenceEmbedings(x['Sentence1']), \n",
    "                                           getSentenceEmbedings(x['Sentence2'])).item(0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 4], [4, 3], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "def get_align(x):\n",
    "    \n",
    "    try:\n",
    "        return x['Alignment'][0]\n",
    "    except:\n",
    "        return []\n",
    "a = {}        \n",
    "a['Alignment'] = [[[10, 4], [4, 3], [2, 2]], [['.', '.'], ['voice', 'sound'], ['high-pitched', 'high-pitched']]]        \n",
    "print get_align(a);        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Alignment'] = df.apply(lambda x: get_align(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment']), axis=1)\n",
    "df['N_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'n'), axis=1)\n",
    "df['V_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'v'), axis=1)\n",
    "df['R_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'r'), axis=1)\n",
    "df['A_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'a'), axis=1)\n",
    "df['P_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'p'), axis=1)\n",
    "df['CD_align_score'] = df.apply(lambda x: get_alignment_score(x['Sentence1'],x['Sentence2'],x['Alignment'], 'cd'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances,euclidean_distances\n",
    "from scipy.stats import pearsonr,kendalltau\n",
    "\n",
    "def get_mahattan(s1,s2):\n",
    "    s1_emb = getSentenceEmbedings(s1)\n",
    "    s2_emb = getSentenceEmbedings(s2)\n",
    "    \n",
    "    m = manhattan_distances(s1_emb,s2_emb)\n",
    "    \n",
    "    return m[0][0]\n",
    "\n",
    "def get_euclidian(s1,s2):\n",
    "    s1_emb = getSentenceEmbedings(s1)\n",
    "    s2_emb = getSentenceEmbedings(s2)\n",
    "    \n",
    "    m = euclidean_distances(s1_emb,s2_emb)\n",
    "    \n",
    "    return m[0][0]\n",
    "\n",
    "def get_pearson_correlation(s1,s2):\n",
    "    s1_emb = getSentenceEmbedings(s1)\n",
    "    s2_emb = getSentenceEmbedings(s2)\n",
    "    try:\n",
    "        m = pearsonr(s1_emb,s2_emb)\n",
    "        return m[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def get_kend_correlation(s1,s2):\n",
    "    s1_emb = getSentenceEmbedings(s1)\n",
    "    s2_emb = getSentenceEmbedings(s2)\n",
    "    \n",
    "    m = kendalltau(s1_emb,s2_emb)\n",
    "    \n",
    "    return m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['man_dis'] = df.apply(lambda x: get_mahattan(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "df['euc_dis'] = df.apply(lambda x: get_euclidian(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "df['pear_dis'] = df.apply(lambda x: get_pearson_correlation(x['Sentence1'], x['Sentence2']), axis=1)\n",
    "df['kend_dis'] = df.apply(lambda x: get_kend_correlation(x['Sentence1'], x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bleu_score'] = df.apply(lambda x:  nltk.translate.bleu_score.sentence_bleu(x['Sentence1'],x['Sentence2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_S1'] = df['Sentence1'].map(len)\n",
    "df['len_S2'] = df['Sentence2'].map(len)\n",
    "    \n",
    "df['S1_diff_S2'] = (df['len_S1'] - df['len_S2'])\n",
    "df['S2_diff_S1'] = (df['len_S2'] - df['len_S1'])\n",
    "\n",
    "df['S1_or_S2'] =  df.apply(lambda x: x['len_S1'] | x['len_S2'], axis=1)\n",
    "df['S1_and_S2'] = df.apply(lambda x: x['len_S1'] & x['len_S2'], axis=1)\n",
    "\n",
    "df['S1_diff_S2_div_len_S2'] = df['S1_diff_S2'] / df['len_S2'].map(float)\n",
    "df['S2_diff_A_div_len_S1'] = df['S2_diff_S1'] / df['len_S1'].map(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "df['pos_S1'] = df['Sentence1'].map(pos_tag)\n",
    "df['pos_S2'] = df['Sentence2'].map(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"test_data_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
