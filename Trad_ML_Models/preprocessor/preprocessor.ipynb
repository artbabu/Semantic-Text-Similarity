{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "from nltk.corpus import wordnet\n",
    "import unicodedata\n",
    "from nltk.tokenize.moses import MosesDetokenizer,MosesTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr =MosesTokenizer()\n",
    "detokenizer = MosesDetokenizer()\n",
    "      \n",
    "\n",
    "def flatten(tokens):\n",
    "    tokens2 = [([x] if isinstance(x,str) else x) for x in tokens]\n",
    "    flattened = list(itertools.chain(*tokens2))\n",
    "    return flattened\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# remove stop words\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def removeStopWords(words):\n",
    "    wordsFiltered = [] \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)            \n",
    "    return wordsFiltered\n",
    "\n",
    "# remove punctuations\n",
    "\n",
    "def token_punct_comma(tokens):\n",
    "    tokens = detokenizer.tokenize(tokens)\n",
    "    return tknzr.tokenize(tokens)\n",
    "\n",
    "def convert_sentence(sentence):\n",
    "    sentence = unicodedata.normalize('NFD', unicode(sentence, \"utf-8\")).encode('ascii', 'ignore')\n",
    "    try:\n",
    "        if sentence :\n",
    "            sentence = unicodedata.normalize('NFD', unicode(sentence, \"utf-8\")).encode('ascii', 'ignore')\n",
    "    except:\n",
    "        print \"------------- Error unicodeData-----------\"\n",
    "        print sentence\n",
    "    tokens = WhitespaceTokenizer().tokenize(sentence.lower())\n",
    "#     comma_tokenized = token_punct_comma(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dictionary of contractions\n",
    "contractions = {\n",
    "  \"'s\": [\"is\"],\n",
    "  \"'re\": [\"are\"],\n",
    "  \"aren't\": [\"are\", \"not\"],\n",
    "  \"can't\": [\"can\", \"not\"],\n",
    "  \"can't've\": [\"can\", \"not\", \"have\"],\n",
    "  \"'cause\": [\"because\"],\n",
    "  \"could've\": [\"could\", \"have\"],\n",
    "  \"couldn't\": [\"could\", \"not\"],\n",
    "  \"couldn't've\": [\"could\", \"not\", \"have\"],\n",
    "  \"didn't\": [\"did\", \"not\"],\n",
    "  \"doesn't\": [\"does\", \"not\"],\n",
    "  \"don't\": [\"do\", \"not\"],\n",
    "  \"hadn't\": [\"had\", \"not\"],\n",
    "  \"hadn't've\": [\"had\", \"not\", \"have\"],\n",
    "  \"hasn't\": [\"has\", \"not\"],\n",
    "  \"haven't\": [\"have\", \"not\"],\n",
    "  \"he'd\": [\"he\", \"would\"],\n",
    "  \"he'd've\": [\"he\", \"would\", \"have\"],\n",
    "  \"he'll\": [\"he\", \"will\"],\n",
    "  \"he'll've\": [\"he\", \"will\", \"have\"],\n",
    "  \"he's\": [\"he\", \"is\"],\n",
    "  \"how'd\": [\"how\", \"did\"],\n",
    "  \"how'd'y\": [\"how\", \"do\", \"you\"],\n",
    "  \"how'll\": [\"how\", \"will\"],\n",
    "  \"how's\": [\"how\", \"is\"],\n",
    "  \"I'd\": [\"I would\"],\n",
    "  \"I'd've\": [\"I\", \"would\", \"have\"],\n",
    "  \"I'll\": [\"I\", \"will\"],\n",
    "  \"I'll've\": [\"I\", \"will\", \"have\"],\n",
    "  \"I'm\": [\"I\", \"am\"],\n",
    "  \"I've\": [\"I\", \"have\"],\n",
    "  \"isn't\": [\"is\", \"not\"],\n",
    "  \"it'd\": [\"it\", \"had\"],\n",
    "  \"it'd've\": [\"it\", \"would\", \"have\"],\n",
    "  \"it'll\": [\"it\", \"will\"],\n",
    "  \"it'll've\": [\"it\", \"will\", \"have\"],\n",
    "  \"it's\": [\"it\", \"is\"],\n",
    "  \"let's\": [\"let\", \"us\"],\n",
    "  \"ma'am\": [\"madam\"],\n",
    "  \"mayn't\": [\"may\", \"not\"],\n",
    "  \"might've\": [\"might\", \"have\"],\n",
    "  \"mightn't\": [\"might\", \"not\"],\n",
    "  \"mightn't've\": [\"might\", \"not\", \"have\"],\n",
    "  \"must've\": [\"must\", \"have\"],\n",
    "  \"mustn't\": [\"must\", \"not\"],\n",
    "  \"mustn't've\": [\"must\", \"not\", \"have\"],\n",
    "  \"needn't\": [\"need\", \"not\"],\n",
    "  \"needn't've\": [\"need\", \"not\", \"have\"],\n",
    "  \"o'clock\": [\"of\", \"the\", \"clock\"],\n",
    "  \"oughtn't\": [\"ought\", \"not\"],\n",
    "  \"oughtn't've\": [\"ought\", \"not\", \"have\"],\n",
    "  \"shan't\": [\"shall\", \"not\"],\n",
    "  \"sha'n't\": [\"shall\", \"not\"],\n",
    "  \"shan't've\": [\"shall\", \"not\", \"have\"],\n",
    "  \"she'd\": [\"she\", \"would\"],\n",
    "  \"she'd've\": [\"she\", \"would\", \"have\"],\n",
    "  \"she'll\": [\"she\", \"will\"],\n",
    "  \"she'll've\": [\"she\", \"shall\", \"have\"],\n",
    "  \"she's\": [\"she\", \"is\"],\n",
    "  \"should've\": [\"should\", \"have\"],\n",
    "  \"shouldn't\": [\"should\", \"not\"],\n",
    "  \"shouldn't've\": [\"should\", \"not\", \"have\"],\n",
    "  \"so've\": [\"so\", \"have\"],\n",
    "  \"so's\": [\"so\", \"as\"],\n",
    "  \"that'd\": [\"that\", \"would\"],\n",
    "  \"that'd've\": [\"that\", \"would\", \"have\"],\n",
    "  \"that's\": [\"that\", \"has\"],\n",
    "  \"there'd\": [\"there\", \"had\"],\n",
    "  \"there'd've\": [\"there\", \"would\", \"have\"],\n",
    "  \"there's\": [\"there\", \"is\"],\n",
    "  \"they'd\": [\"they\", \"had \"],\n",
    "  \"they'd've\": [\"they\", \"would\", \"have\"],\n",
    "  \"they'll\": [\"they\", \"will\"],\n",
    "  \"they'll've\": [\"they\", \"will\", \"have\"],\n",
    "  \"they're\": [\"they\", \"are\"],\n",
    "  \"they've\": [\"they\", \"have\"],\n",
    "  \"to've\": [\"to\", \"have\"],\n",
    "  \"wasn't\": [\"was\", \"not\"],\n",
    "  \"we'd\": [\"we\", \"had\"],\n",
    "  \"we'd've\": [\"we\", \"would\", \"have\"],\n",
    "  \"we'll\": [\"we\", \"will\"],\n",
    "  \"we'll've\": [\"we\", \"will\", \"have\"],\n",
    "  \"we're\": [\"we\", \"are\"],\n",
    "  \"we've\": [\"we\", \"have\"],\n",
    "  \"weren't\": [\"were\", \"not\"],\n",
    "  \"what'll\": [\"what\", \"will\"],\n",
    "  \"what'll've\": [\"what\", \"will\", \"have\"],\n",
    "  \"what're\": [\"what\", \"are\"],\n",
    "  \"what's\": [\"what\", \"is\"],\n",
    "  \"what've\": [\"what have\"],\n",
    "  \"when's\": [\"when\", \"is\"],\n",
    "  \"when've\": [\"when\", \"have\"],\n",
    "  \"where'd\": [\"where\", \"did\"],\n",
    "  \"where's\": [\"where\", \"is\"],\n",
    "  \"where've\": [\"where\", \"have\"],\n",
    "  \"who'll\": [\"who\", \"will\"],\n",
    "  \"who'll've\": [\"who\", \"will\", \"have\"],\n",
    "  \"who's\": [\"who\", \"is\"],\n",
    "  \"who've\": [\"who\", \"have\"],\n",
    "  \"why's\": [\"why\", \"is\"],\n",
    "  \"why've\": [\"why\", \"have\"],\n",
    "  \"will've\": [\"will\", \"have\"],\n",
    "  \"won't\": [\"will\", \"not\"],\n",
    "  \"won't've\": [\"will\", \"not\", \"have\"],\n",
    "  \"would've\": [\"would\", \"have\"],\n",
    "  \"wouldn't\": [\"would\", \"not\"],\n",
    "  \"wouldn't've\": [\"would\", \"not\", \"have\"],\n",
    "  \"y'all\": [\"you\", \"all\"],\n",
    "  \"y'all'd\": [\"you\", \"all\", \"would\"],\n",
    "  \"y'all'd've\": [\"you\", \"all\", \"would\", \"have\"],\n",
    "  \"y'all're\": [\"you\", \"all\", \"are\"],\n",
    "  \"y'all've\": [\"you\", \"all\", \"have\"],\n",
    "  \"you'd\": [\"you\", \"had\"],\n",
    "  \"you'd've\": [\"you\", \"would\", \"have\"],\n",
    "  \"you'll\": [\"you\", \"will\"],\n",
    "  \"you'll've\": [\"you\", \"will\", \"have\"],\n",
    "  \"you're\": [\"you\", \"are\"],\n",
    "  \"you've\": [\"you\", \"have\"]\n",
    "}\n",
    "\n",
    "def expand_contraction(s, contractions_dict = contractions):\n",
    "    return contractions_dict.get(s)\n",
    "\n",
    "def find_contraction(s):\n",
    "    if '\\'' in s:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def replace_contraction(s):\n",
    "    if(find_contraction(s)):\n",
    "        expanded = expand_contraction(s)\n",
    "    return expanded\n",
    "\n",
    "def expand(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        if '\\'' in tokens[i]:\n",
    "            expandedtoken = replace_contraction(tokens[i])\n",
    "            if(expandedtoken != None):\n",
    "                tokens[i] = expandedtoken\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def tag_pos (sentence):\n",
    "    tagged = pos_tag(sentence)\n",
    "    return tagged\n",
    "\n",
    "def replace_base (sentence):\n",
    "    tagged = tag_pos(sentence)\n",
    "    replaced = []\n",
    "    for i in range(len(tagged)):\n",
    "        token = tagged[i]\n",
    "        word = token[0]\n",
    "        pos = get_wordnet_pos(token[1])\n",
    "        try:\n",
    "            \n",
    "            base = wn.morphy(word, pos)\n",
    "        except:\n",
    "            print \"-------------------- Error --------------------\"\n",
    "            print word\n",
    "        if(base != None):\n",
    "            replaced.append(base)\n",
    "        else:\n",
    "            replaced.append(word)\n",
    "    return replaced\n",
    "\n",
    "def synonyms_word (word):\n",
    "    wordlemma = []\n",
    "    synsets = wn.synsets(word)\n",
    "    for synset in synsets:\n",
    "        lemma = [str(lemma.name()) for lemma in synset.lemmas()]\n",
    "        wordlemma.append(lemma)\n",
    "    flattened = flatten(wordlemma)\n",
    "    unique = list(set(flattened))\n",
    "    return unique\n",
    "\n",
    "def synonyms_sentence (sentence):\n",
    "    syn = []\n",
    "    for word in sentence:\n",
    "        syn.append(synonyms_word(word))\n",
    "        \n",
    "    return syn\n",
    "\n",
    "def replace_syn(x):\n",
    "    \n",
    "    for i in range(len(x['Sentence2'])):\n",
    "        a = synonyms_word(x['Sentence2'][i])\n",
    "        for replacew in x['Sentence1']:\n",
    "            b = synonyms_word(replacew)\n",
    "            if(not set(a).isdisjoint(b)):\n",
    "                x['Sentence2'][i] = replacew\n",
    "                break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.parse.stanford import GenericStanfordParser\n",
    "\n",
    "# g = GenericStanfordParser()\n",
    "# print g.tagged_parse(nltk.pos_tag('Four men died in an accident.'))\n",
    "from nltk.tokenize.moses import MosesDetokenizer,MosesTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr =MosesTokenizer()\n",
    "detokenizer = MosesDetokenizer()\n",
    "def sentence_tokenise(s):\n",
    "    s = detokenizer.detokenize(s, return_str=True)\n",
    "            \n",
    "    return tknzr.tokenize(s)        \n",
    "\n",
    "\n",
    "def preprocessFile(inputFile):\n",
    "    \n",
    "    df = pd.read_pickle(inputFile)\n",
    "\n",
    "    #0.convert the contraction & lowercase\n",
    "    df['Sentence1'] = df.apply(lambda x: convert_sentence(x['Sentence1']), axis=1)\n",
    "    df['Sentence2'] = df.apply(lambda x: convert_sentence(x['Sentence2']), axis=1)\n",
    "\n",
    "    \n",
    "    #1. no stopwords\n",
    "    df['Sentence1'] = df.apply(lambda x: removeStopWords(x['Sentence1']), axis=1)\n",
    "    df['Sentence2'] = df.apply(lambda x: removeStopWords(x['Sentence2']), axis=1)\n",
    "\n",
    "    \n",
    "#     #2.lemmatize to base\n",
    "    df['Sentence1'] = df.apply(lambda x: replace_base(x['Sentence1']), axis=1)\n",
    "    df['Sentence2'] = df.apply(lambda x: replace_base(x['Sentence2']), axis=1)\n",
    "\n",
    "    #3.replace synonyms\n",
    "    df = df.apply(lambda x: replace_syn(x), axis=1)\n",
    "    \n",
    "    df['Sentence1'] = df.apply(lambda x: sentence_tokenise(x['Sentence1']), axis=1)\n",
    "    df['Sentence2'] = df.apply(lambda x: sentence_tokenise(x['Sentence2']), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "# def encode(s):\n",
    "#     return s.encode('utf-8')\n",
    "\n",
    "\n",
    "# df = preprocessFile('../Data/backup/Train_Data')\n",
    "# df.to_pickle(\"Preprocessed_ML_TRAIN.pickle\")\n",
    "# df.to_excel(\"Preprocessed_ML_TRAIN.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czesc\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "text = u'Cześć'\n",
    "print unicodedata.normalize('NFD', text).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessFile('../../data/Pandas_Pickle/STS/SICK/SICK_test_sr')\n",
    "df.to_pickle(\"Preprocessed_ML_TEST.pickle\")\n",
    "df.to_excel(\"Preprocessed_ML_TEST.xlsx\")\n",
    "\n",
    "df = preprocessFile('../../data/Pandas_Pickle/STS/SICK/SICK_test_rte')\n",
    "df.to_pickle(\"Preprocessed_ML_TEST_RTE.pickle\")\n",
    "df.to_excel(\"Preprocessed_ML_TEST_RTE.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
